{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "This Python Notebook will show a number of collaborative filtering techniques, applied to the Movielens 100K dataset. The components required are distributed across different files (e.g., users, items, ratings, etc.). \n",
    "\n",
    "- The most prominent approach to generate recommendations\n",
    "- People who agreed in their subjective evaluations in the past are likely to agree again in the future.\n",
    "- The filtering decision in CF based on human and not machine analysis of the content.\n",
    "    - Memory based CF: operates over the entire user database to make predictions, by obtaining similar relationship between user or items according to user-item rating matrix and then recommeds the items that are highly rated by similar users for the active user. (user based, items based)\n",
    "    - Model base CF:  requires a learning phase in advance for finding out the optimal\n",
    "model parameters before making a recommendation, after the learning phase is finished, the model\n",
    "based RS, easily predict the ratings of the active user.\n",
    "    \n",
    "- we will focus mainly on the User based approach: used the users database to estimate or learn a model, which is then used for prediction. \n",
    "     \n",
    "     **Procedure**: Given an \"active user\" (Alice) and an item *i* not yet seen by Alice:\n",
    "        \n",
    "        1.  Find a set of users (peers/ nearest neighbors) who like the same items as Alice in the pase **and**  who have rated item *i*.\n",
    "        2.  User there ratings (e.g, their average) to predict whether Alice will like item *i*\n",
    "        3.  Do this for all items Alice has not seen and recommend the best-rated\n",
    "\n",
    "    - The same procedure can be applied on item-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Framework - Standard approach\n",
    "#### Pre-processing the dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we import the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the u.user file into a dataframe\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code'] # define names for the columns\n",
    "\n",
    "users = pd.read_csv('./u.user', sep='|', names=u_cols,      # read the data\n",
    " encoding='latin-1')\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the u.item file into a dataframe\n",
    "i_cols = ['movie_id', 'title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "movies = pd.read_csv('u.item', sep='|', names=i_cols, encoding='latin-1')\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all information except Movie ID and title :\n",
    "movies = movies[['movie_id', 'title']] # specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the u.data file into a dataframe\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "\n",
    "ratings = pd.read_csv('./u.data', sep='\\t', names=r_cols,\n",
    " encoding='latin-1')\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the timestamp column\n",
    "ratings = ratings.drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating into training and test data\n",
    "Eventually, we would like to generate a prediction model based on our rating data, which runs from 1 to 5. While a dichotomous classification model (true/false) would not care about the magnitude of error in a prediction (e.g., if the true value is 5, 1 and 4 are bad predicitions), we would like the model to be 'punished' in line with a regular regression model (e.g., a prediction of 4 for the true of 5 is better than a prediction of 2). \n",
    "\n",
    "To evaluate the data like that, we will first need to separate our training and test data. In this example, we split 75% of the data in a training set, and 25% of the data in a validation. You are of course free to change these parameters below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method below\n",
    "The split is done in a slightly 'hacky' way: we assume that the user_id is the target variable (or Y) and that our ratings dataframe comprises the predictor variables (or x). We will then pass these two varaibles into scikit-learn's train_test_split function and stratify it along y. This ensures that the proportion of each class is the same in both the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the train_test_split function\n",
    "from sklearn.model_selection import ## your code \n",
    "\n",
    "# Assign X as the original ratings dataframe and y as the user_id column of ratings.\n",
    "X =  # your code \n",
    "y = ratings['user_id']  \n",
    "\n",
    "# Split into training and test datasets, stratified along user_id with 25% of data as a Training data\n",
    "''' train_test_split(\n",
    "        Predictor variables (X),\n",
    "        Outcome variable (label: y),\n",
    "        test_size,\n",
    "        stratify:  split data in a stratified way,\n",
    "        random_state:  reproducibility variable (to find the same results)\n",
    ") '''\n",
    "\n",
    "X_train, X_test, y_train, y_test = # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Root Mean Squared Error is the most common metric (minimizing error) between the predicted values and actual values. \n",
    "$$ RMSE = \\sqrt{\\frac{\\sum_{(u,i,r)\\in R}{(\\hat{r_{u,i}} - r_{u,i})^2}}{|R|}}$$\n",
    "\n",
    "We will first use this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the mean_squared_error function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Function that computes the root mean squared error (or RMSE)\n",
    "def rmse(y_true, y_pred):\n",
    "    return ## your code (mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model to always return 3.\n",
    "def baseline(user_id, movie_id):\n",
    "    return 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute the RMSE score obtained on the testing set by a model\n",
    "def score(cf_model):\n",
    "    \n",
    "    #Construct a list of user-movie tuples from the testing dataset\n",
    "    id_pairs = zip(X_test['user_id'], X_test['movie_id'])\n",
    "    \n",
    "    #Predict the rating for every user-movie tuple\n",
    "    y_pred = np.array([cf_model(user, movie) for (user, movie) in id_pairs])\n",
    "    print(y_pred)\n",
    "    \n",
    "    #Extract the actual ratings given by the users in the test data\n",
    "    y_true = np.array(X_test['rating'])\n",
    "    \n",
    "    #Return the final RMSE score\n",
    "    return rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RMSE scoce between of our  baseline model that predict 3 for all ratings \n",
    "\n",
    "score(## your code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## User Based Collaborative Filtering\n",
    "\n",
    "### Ratings Matrix\n",
    "The columns represent the movies, the rows represent the users. Each cell is a rating given a user i to a movie j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the ratings matrix using pivot_table function\n",
    "r_matrix = X_train.pivot_table(values='rating', index='user_id', columns='movie_id')\n",
    "\n",
    "r_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean\n",
    "First, we will build the simples collaborative filter possible. We will compute the means of each movie in our training dataset. In doing so, we assume equal weight of each user in determining the rating (which is of course not very accurate, RIGHT?)\n",
    "\n",
    "If there are no ratings available in either the training or test dataset, we will assume the absolute mean value for that movie: 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Based Collaborative Filter using Mean Ratings\n",
    "def cf_user_mean(user_id, movie_id):\n",
    "    \n",
    "    #Check if movie_id exists in r_matrix\n",
    "    if movie_id in r_matrix:\n",
    "        #Compute the mean of all the ratings given to the movie\n",
    "        mean_rating = r_matrix[movie_id].mean()\n",
    "    \n",
    "    else:\n",
    "        #Default to a rating of 3.0 in the absence of any information\n",
    "        mean_rating = 3.0\n",
    "    \n",
    "    return mean_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute RMSE for the Mean model\n",
    "score(cf_user_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Mean\n",
    "The weighted mean is computed by multiplying ratings with some kind of weight. As we've learned, this is a similarity score between two users when we are performing user-user CF.\n",
    "\n",
    "The rating can be predicted by \n",
    "\n",
    "**r(u,m) = (similarity between_two_users * rating_of_user_for_item) / (Euclidean  length  of similarity between two users)**.\n",
    "\n",
    "For the sake of the exercise, we will focus on cosine similarity. Since Scikit-learns' cosine similarity can't handle missing values (i.e., NaN), we need to convert them to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dummy ratings matrix with all null values imputed to 0\n",
    "r_matrix_dummy = r_matrix.copy().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_score \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Compute the cosine similarity matrix using the dummy ratings matrix\n",
    "cosine_sim = cosine_similarity(r_matrix_dummy, r_matrix_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "      <th>937</th>\n",
       "      <th>938</th>\n",
       "      <th>939</th>\n",
       "      <th>940</th>\n",
       "      <th>941</th>\n",
       "      <th>942</th>\n",
       "      <th>943</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118076</td>\n",
       "      <td>0.029097</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.264677</td>\n",
       "      <td>0.312419</td>\n",
       "      <td>0.308729</td>\n",
       "      <td>0.224269</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.286411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308475</td>\n",
       "      <td>0.055872</td>\n",
       "      <td>0.197862</td>\n",
       "      <td>0.131367</td>\n",
       "      <td>0.152449</td>\n",
       "      <td>0.084456</td>\n",
       "      <td>0.293293</td>\n",
       "      <td>0.056765</td>\n",
       "      <td>0.103536</td>\n",
       "      <td>0.326491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.118076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099097</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>0.034279</td>\n",
       "      <td>0.152789</td>\n",
       "      <td>0.086705</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.068940</td>\n",
       "      <td>0.092399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086927</td>\n",
       "      <td>0.259636</td>\n",
       "      <td>0.289092</td>\n",
       "      <td>0.318824</td>\n",
       "      <td>0.149105</td>\n",
       "      <td>0.186347</td>\n",
       "      <td>0.168034</td>\n",
       "      <td>0.106748</td>\n",
       "      <td>0.136796</td>\n",
       "      <td>0.080358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029097</td>\n",
       "      <td>0.099097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252131</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.062539</td>\n",
       "      <td>0.039767</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040918</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.096993</td>\n",
       "      <td>0.109631</td>\n",
       "      <td>0.092574</td>\n",
       "      <td>0.018987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>0.252131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.078812</td>\n",
       "      <td>0.095354</td>\n",
       "      <td>0.059498</td>\n",
       "      <td>0.053879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024226</td>\n",
       "      <td>0.050703</td>\n",
       "      <td>0.056561</td>\n",
       "      <td>0.107294</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.142798</td>\n",
       "      <td>0.097066</td>\n",
       "      <td>0.015176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.264677</td>\n",
       "      <td>0.034279</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202843</td>\n",
       "      <td>0.299619</td>\n",
       "      <td>0.163724</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>0.153021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262547</td>\n",
       "      <td>0.048524</td>\n",
       "      <td>0.048312</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>0.091910</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.156172</td>\n",
       "      <td>0.115842</td>\n",
       "      <td>0.124297</td>\n",
       "      <td>0.267574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.312419</td>\n",
       "      <td>0.152789</td>\n",
       "      <td>0.062539</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.202843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375963</td>\n",
       "      <td>0.131795</td>\n",
       "      <td>0.110944</td>\n",
       "      <td>0.400758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287549</td>\n",
       "      <td>0.080312</td>\n",
       "      <td>0.162988</td>\n",
       "      <td>0.182856</td>\n",
       "      <td>0.114262</td>\n",
       "      <td>0.092090</td>\n",
       "      <td>0.261859</td>\n",
       "      <td>0.097606</td>\n",
       "      <td>0.206104</td>\n",
       "      <td>0.187637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.308729</td>\n",
       "      <td>0.086705</td>\n",
       "      <td>0.039767</td>\n",
       "      <td>0.078812</td>\n",
       "      <td>0.299619</td>\n",
       "      <td>0.375963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.211282</td>\n",
       "      <td>0.107795</td>\n",
       "      <td>0.328923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290002</td>\n",
       "      <td>0.074170</td>\n",
       "      <td>0.094619</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>0.115620</td>\n",
       "      <td>0.100625</td>\n",
       "      <td>0.233843</td>\n",
       "      <td>0.039199</td>\n",
       "      <td>0.224227</td>\n",
       "      <td>0.296332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.224269</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.095354</td>\n",
       "      <td>0.163724</td>\n",
       "      <td>0.131795</td>\n",
       "      <td>0.211282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037040</td>\n",
       "      <td>0.183375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165008</td>\n",
       "      <td>0.066843</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.068759</td>\n",
       "      <td>0.087159</td>\n",
       "      <td>0.129381</td>\n",
       "      <td>0.188662</td>\n",
       "      <td>0.121223</td>\n",
       "      <td>0.083910</td>\n",
       "      <td>0.273238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.068940</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.059498</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>0.110944</td>\n",
       "      <td>0.107795</td>\n",
       "      <td>0.037040</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101710</td>\n",
       "      <td>0.034568</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.052699</td>\n",
       "      <td>0.107486</td>\n",
       "      <td>0.055766</td>\n",
       "      <td>0.070065</td>\n",
       "      <td>0.088281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.286411</td>\n",
       "      <td>0.092399</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>0.053879</td>\n",
       "      <td>0.153021</td>\n",
       "      <td>0.400758</td>\n",
       "      <td>0.328923</td>\n",
       "      <td>0.183375</td>\n",
       "      <td>0.155435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278558</td>\n",
       "      <td>0.049310</td>\n",
       "      <td>0.153506</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.060088</td>\n",
       "      <td>0.033686</td>\n",
       "      <td>0.197107</td>\n",
       "      <td>0.085402</td>\n",
       "      <td>0.118945</td>\n",
       "      <td>0.162538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id       1         2         3         4         5         6         7    \\\n",
       "user_id                                                                         \n",
       "1        1.000000  0.118076  0.029097  0.011628  0.264677  0.312419  0.308729   \n",
       "2        0.118076  1.000000  0.099097  0.107680  0.034279  0.152789  0.086705   \n",
       "3        0.029097  0.099097  1.000000  0.252131  0.026893  0.062539  0.039767   \n",
       "4        0.011628  0.107680  0.252131  1.000000  0.000000  0.045543  0.078812   \n",
       "5        0.264677  0.034279  0.026893  0.000000  1.000000  0.202843  0.299619   \n",
       "6        0.312419  0.152789  0.062539  0.045543  0.202843  1.000000  0.375963   \n",
       "7        0.308729  0.086705  0.039767  0.078812  0.299619  0.375963  1.000000   \n",
       "8        0.224269  0.078864  0.089474  0.095354  0.163724  0.131795  0.211282   \n",
       "9        0.026017  0.068940  0.078162  0.059498  0.038474  0.110944  0.107795   \n",
       "10       0.286411  0.092399  0.037670  0.053879  0.153021  0.400758  0.328923   \n",
       "\n",
       "user_id       8         9         10   ...       934       935       936  \\\n",
       "user_id                                ...                                 \n",
       "1        0.224269  0.026017  0.286411  ...  0.308475  0.055872  0.197862   \n",
       "2        0.078864  0.068940  0.092399  ...  0.086927  0.259636  0.289092   \n",
       "3        0.089474  0.078162  0.037670  ...  0.040918  0.019031  0.065417   \n",
       "4        0.095354  0.059498  0.053879  ...  0.024226  0.050703  0.056561   \n",
       "5        0.163724  0.038474  0.153021  ...  0.262547  0.048524  0.048312   \n",
       "6        0.131795  0.110944  0.400758  ...  0.287549  0.080312  0.162988   \n",
       "7        0.211282  0.107795  0.328923  ...  0.290002  0.074170  0.094619   \n",
       "8        1.000000  0.037040  0.183375  ...  0.165008  0.066843  0.058766   \n",
       "9        0.037040  1.000000  0.155435  ...  0.011708  0.000000  0.101710   \n",
       "10       0.183375  0.155435  1.000000  ...  0.278558  0.049310  0.153506   \n",
       "\n",
       "user_id       937       938       939       940       941       942       943  \n",
       "user_id                                                                        \n",
       "1        0.131367  0.152449  0.084456  0.293293  0.056765  0.103536  0.326491  \n",
       "2        0.318824  0.149105  0.186347  0.168034  0.106748  0.136796  0.080358  \n",
       "3        0.055373  0.086503  0.018418  0.096993  0.109631  0.092574  0.018987  \n",
       "4        0.107294  0.098892  0.000000  0.132900  0.142798  0.097066  0.015176  \n",
       "5        0.022202  0.091910  0.066000  0.156172  0.115842  0.124297  0.267574  \n",
       "6        0.182856  0.114262  0.092090  0.261859  0.097606  0.206104  0.187637  \n",
       "7        0.084235  0.115620  0.100625  0.233843  0.039199  0.224227  0.296332  \n",
       "8        0.068759  0.087159  0.129381  0.188662  0.121223  0.083910  0.273238  \n",
       "9        0.034568  0.045002  0.052699  0.107486  0.055766  0.070065  0.088281  \n",
       "10       0.065471  0.060088  0.033686  0.197107  0.085402  0.118945  0.162538  \n",
       "\n",
       "[10 rows x 943 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert into pandas dataframe \n",
    "cosine_sim = pd.DataFrame(cosine_sim, index=r_matrix.index, columns=r_matrix.index)\n",
    "\n",
    "cosine_sim.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity matrix above, we are now in the position to efficiently calculate the weighted mean scores for this model. However, implementing this model in code is slightly more complex than the regular mean above, for that we only need to consider cosine similarity score that have a non-null rating. Hence, we need to avoid all users that have not rated a certain movie m. To do this, we need to double check the similarity score that we have with the rating matrix of earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Based Collaborative Filter using Weighted Mean Ratings\n",
    "def cf_user_wmean(user_id, movie_id):\n",
    "    \n",
    "    #Check if movie_id exists in r_matrix\n",
    "    if movie_id in r_matrix:\n",
    "        \n",
    "        #Get the similarity scores for the user in question with every other user\n",
    "        sim_scores = cosine_sim[user_id]\n",
    "        \n",
    "        #Get the user ratings for the movie in question\n",
    "        m_ratings = r_matrix[movie_id]\n",
    "        \n",
    "        #Extract the indices containing NaN in the m_ratings series\n",
    "        idx = m_ratings[m_ratings.isnull()].index\n",
    "        \n",
    "        #Drop the NaN values from the m_ratings Series\n",
    "        m_ratings = m_ratings.dropna()\n",
    "        \n",
    "        #Drop the corresponding cosine scores from the sim_scores series\n",
    "        sim_scores = sim_scores.drop(idx)\n",
    "        \n",
    "        #Compute the final weighted mean\n",
    "        wmean_rating = np.dot(sim_scores, m_ratings)/ sim_scores.sum()\n",
    "        if SimScore == 0:\n",
    "                SimScore = 1\n",
    "        wmean_rating = np.dot(sim_scores, m_ratings) / SimScore\n",
    "    \n",
    "    else:\n",
    "        #Default to a rating of 3.0 in the absence of any information\n",
    "        wmean_rating = 3.0\n",
    "    \n",
    "    return wmean_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only dealing with positive ratings, we do not need to build in a modulus/mode function. As you will see, the improvement in RMSE is very small given the longer runtime of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the RMSE score for this model\n",
    "score(cf_user_wmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics\n",
    "Demographic collaborative filters rely on the intuition that users with similar backgrounds (ages, sex, etc.) are more likely to have similar tastes. This means that we do not need to take all ratings of all users into account, but only the ratings of those that are relevant to another user. \n",
    "\n",
    "The first demographic filter we will build simply takes the gender of the user, compute the (weighted) mean rating of a movie by that particular gender, and return that as the predicted value. To obtain this information, we need to merge our predictor set with the demographic dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>889</td>\n",
       "      <td>684</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>889</td>\n",
       "      <td>279</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>889</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>889</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>889</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>78704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  age sex  occupation zip_code\n",
       "0      889       684       2   24   M  technician    78704\n",
       "1      889       279       2   24   M  technician    78704\n",
       "2      889        29       3   24   M  technician    78704\n",
       "3      889       190       3   24   M  technician    78704\n",
       "4      889       232       3   24   M  technician    78704"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the original users dataframe with the training set \n",
    "merged_df = pd.merge(X_train, users)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean rating given by each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean rating of every movie by gender\n",
    "gender_mean = merged_df[['movie_id', 'sex', 'rating']].groupby(['movie_id', 'sex'])['rating'].mean()\n",
    "\n",
    "## gender_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the index of the users dataframe to the user_id\n",
    "users = users.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Based Collaborative Filter using Mean Ratings\n",
    "def cf_gender(user_id, movie_id):\n",
    "    \n",
    "    #Check if movie_id exists in r_matrix (or training set)\n",
    "    if movie_id in r_matrix:\n",
    "        #Identify the gender of the user\n",
    "        gender = users.loc[user_id]['sex']\n",
    "        \n",
    "        #Check if the gender has rated the movie\n",
    "        if gender in gender_mean[movie_id]:\n",
    "            \n",
    "            #Compute the mean rating given by that gender to the movie\n",
    "            gender_rating = gender_mean[movie_id][gender]\n",
    "        \n",
    "        else:\n",
    "            gender_rating = 3.0\n",
    "    \n",
    "    else:\n",
    "        #Default to a rating of 3.0 in the absence of any information\n",
    "        gender_rating = 3.0\n",
    "    \n",
    "    return gender_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the RMSE Score\n",
    "score(cf_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RMSE is slightly worse than for the other approaches, we can assume that gender is probably not a good predictor of movie taste. Let's to expand, by using gender and occupation simultaneously. Because... doctors must doctor movies, etc.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the mean rating by gender and occupation\n",
    "gen_occ_mean = merged_df[['sex', 'rating', 'movie_id', 'occupation']].pivot_table(\n",
    "    values='rating', index='movie_id', columns=['occupation', 'sex'], aggfunc='mean')\n",
    "\n",
    "gen_occ_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivottable is another way of using the groupby command, but is slightly more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender and Occupation Based Collaborative Filter using Mean Ratings\n",
    "def cf_gen_occ(user_id, movie_id):\n",
    "    \n",
    "    #Check if movie_id exists in gen_occ_mean\n",
    "    if movie_id in gen_occ_mean.index:\n",
    "        \n",
    "        #Identify the user\n",
    "        user = users.loc[user_id]\n",
    "        \n",
    "        #Identify the gender and occupation\n",
    "        gender = user['sex']\n",
    "        occ = user['occupation']\n",
    "        \n",
    "        #Check if the occupation has rated the movie\n",
    "        if occ in gen_occ_mean.loc[movie_id]:\n",
    "            \n",
    "            #Check if the gender has rated the movie\n",
    "            if gender in gen_occ_mean.loc[movie_id][occ]:\n",
    "                \n",
    "                #Extract the required rating\n",
    "                rating = gen_occ_mean.loc[movie_id][occ][gender]\n",
    "                \n",
    "                #Default to 3.0 if the rating is null\n",
    "                if np.isnan(rating):\n",
    "                    rating = 3.0\n",
    "                \n",
    "                return rating\n",
    "            \n",
    "    #Return the default rating    \n",
    "    return 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE Score\n",
    "score(cf_gen_occ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this has been the worst improvement of the baseline so far. Apparently, this is not the way forward to improve the model accuracy, but you are free to experiment with different demographic characteristics! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based collaborative filtering\n",
    "You could also focus on item-item CF and compute the pairwise similarity of every item in the inventory. We will again apply a weighted mean function to come up with our model, as we expect users to give similar ratings to movies for which we have computed that they are similar.\n",
    "\n",
    "## Model Based Approaches\n",
    "The previous examples have been memory-based (think about why :-). The upcoming methods will make use of model-based approaches, in the sense that we are actually going to apply machine learning!\n",
    "\n",
    "The previous example with demographics was a bit too simplistic. Now, we are going to move beyond the metadata that we have by using cluster algorithms such as k-means to group users into a cluster and then to take only the users from the same cluster into consideration when predicting ratings.\n",
    "Now, we are using kNN. The steps are as follows:\n",
    "1. Find the k-nearest neightbors of u who have rated movie m\n",
    "2. Output the average rating of the k users for the movie m.\n",
    "\n",
    "This simple approach happens to be among the most popular algorithms in use. We will implement this using an extremely popular and robust library called 'Surprise', which is also a scikit. Surprise is an acronym of 'Simple Python Recommendation System Engine'. You might need to download it first to it to work! Use the following command in your command prompt environment:\n",
    "\n",
    "sudo pip3 install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required classes and methods from the surprise library\n",
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "#Define a Reader object\n",
    "#The Reader object helps in parsing the file or dataframe containing ratings\n",
    "reader = Reader()\n",
    "\n",
    "#Create the dataset to be used for building the filter\n",
    "data = Dataset.load_from_df(ratings, reader)\n",
    "\n",
    "#Define the algorithm object; in this case kNN\n",
    "knn = KNNBasic()\n",
    "\n",
    "#Evaluate the performance in terms of RMSE using  cross validation\n",
    "\n",
    "cross_validate(knn, data, measures=['RMSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, you can compute the mean RMSE by averaging the test_rmse values. The result is much better than our previous approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition\n",
    "Remember from the previous slides what this is? It aims to reduce the number of dimensions in your data, as not every rating should be considered as a unique dimension. It is an advanced version of Principal Component AnalysisL You separate a single user-item rating matrix into three parts: a user rating part (user preferences for each relevant dimension), an item rating part (how well an item scores on each dimension), and a weights part (how relevant each dimension is). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SVD\n",
    "from surprise import SVD\n",
    "\n",
    "#Define the SVD algorithm object\n",
    "svd = SVD()\n",
    "\n",
    "#Evaluate the performance in terms of RMSE\n",
    "cross_validate(svd, data, measures=['RMSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, we find a better RMSE score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise Library: https://surpriselib.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
